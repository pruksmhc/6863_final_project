{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "import datetime\n",
    "import csv\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 400  \n",
    "latent_dim = 10\n",
    "num_samples = 45133\n",
    "\n",
    "# This flag should be set to 1 for the classification style training\n",
    "# Classification based task is when the network is expected to output\n",
    "# 's' for the input 'apple' and 'es' for 'torch'.\n",
    "# When the classfication flag is set to 0 then the network is expected to \n",
    "# output 'apples' for input 'apple' and 'torches' for input 'torch'. \n",
    "classification_flag = 1\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "data_path = './all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "indices = np.array([i for i in range(len(input_texts))])\n",
    "random.shuffle(indices)\n",
    "input_texts = [input_texts[i] for i in indices]\n",
    "target_texts = [target_texts[i] for i in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28441 19622 25667 ...,  2653 27562 25247]\n"
     ]
    }
   ],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of s_words:  37862\n",
      "Number of es_words:  3297\n",
      "Number of ies_words:  3457\n",
      "Number of other_words:  517\n",
      "Total words:  45133\n"
     ]
    }
   ],
   "source": [
    "s_words = []\n",
    "es_words = []\n",
    "ies_words = []\n",
    "others = []\n",
    "for i in range(len(input_texts)):\n",
    "    input_sample = input_texts[i]\n",
    "    output_sample = target_texts[i][1:-1]\n",
    "\n",
    "    if (input_sample[-2:] == 'ch' or input_sample[-2:] == 'sh' or input_sample[-1:] == 'x' or \\\n",
    "        input_sample[-1:] == 's' or input_sample[-1:] == 'z') and output_sample[-2:] == 'es':\n",
    "        es_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\tes\\n'\n",
    "    elif input_sample[-1:] == 'y' and output_sample[-3:] == 'ies':\n",
    "        ies_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ties\\n'\n",
    "    elif output_sample[-1:] == 's':\n",
    "        s_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ts\\n'\n",
    "    else:\n",
    "        others.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\t\\n'\n",
    "\n",
    "            \n",
    "print(\"Number of s_words: \", len(s_words))\n",
    "print(\"Number of es_words: \", len(es_words))\n",
    "print(\"Number of ies_words: \", len(ies_words))\n",
    "print(\"Number of other_words: \", len(others))\n",
    "print(\"Total words: \", len(s_words) + len(es_words) + len(ies_words) + len(others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tspeedups\\n',\n",
       " '\\trhizopogons\\n',\n",
       " '\\tindorsements\\n',\n",
       " '\\tpajamas\\n',\n",
       " '\\tmicrometries\\n',\n",
       " '\\tnews\\n',\n",
       " '\\tnagas\\n',\n",
       " '\\tschnappss\\n',\n",
       " '\\tcompassionates\\n',\n",
       " '\\toesophagitis\\n',\n",
       " '\\tjibs\\n',\n",
       " '\\tmacambas\\n',\n",
       " '\\tetherises\\n',\n",
       " '\\treflations\\n',\n",
       " '\\ttwirls\\n',\n",
       " '\\thaydns\\n',\n",
       " '\\tmomotuss\\n',\n",
       " '\\tcestidaes\\n',\n",
       " '\\tkennewicks\\n',\n",
       " '\\tsinclairs\\n',\n",
       " '\\tpackagings\\n',\n",
       " '\\tberserkers\\n',\n",
       " '\\tadrss\\n',\n",
       " '\\tcharacins\\n',\n",
       " '\\tpenalties\\n',\n",
       " '\\tsynthesizers\\n',\n",
       " '\\ttransfigurations\\n',\n",
       " '\\thalakas\\n',\n",
       " '\\tzitis\\n',\n",
       " '\\texcavates\\n',\n",
       " '\\tmontezs\\n',\n",
       " '\\tdevilries\\n',\n",
       " '\\tjingoists\\n',\n",
       " '\\tprotoes\\n',\n",
       " '\\tprofundities\\n',\n",
       " '\\tdichotomisations\\n',\n",
       " '\\tbesseras\\n',\n",
       " '\\tcrowberries\\n',\n",
       " '\\twhitewaters\\n',\n",
       " '\\teffeminises\\n',\n",
       " '\\tdiscoverers\\n',\n",
       " '\\tcordays\\n',\n",
       " '\\tsarahs\\n',\n",
       " '\\thubriss\\n',\n",
       " '\\tmushes\\n',\n",
       " '\\trothkoes\\n',\n",
       " '\\teiders\\n',\n",
       " '\\tfibs\\n',\n",
       " '\\tperjuries\\n',\n",
       " '\\tintuitionists\\n',\n",
       " '\\theavens\\n',\n",
       " '\\tiwoes\\n',\n",
       " '\\ttriavils\\n',\n",
       " '\\trusticates\\n',\n",
       " '\\tstarches\\n',\n",
       " '\\tperniss\\n',\n",
       " '\\tcowlicks\\n',\n",
       " '\\trancors\\n',\n",
       " '\\tvenushairs\\n',\n",
       " '\\thippodamias\\n',\n",
       " '\\tdreams\\n',\n",
       " '\\tpeachies\\n',\n",
       " '\\tscaphocephalies\\n',\n",
       " '\\tstaggerers\\n',\n",
       " '\\thappiness\\n',\n",
       " '\\tdistiches\\n',\n",
       " '\\tcumquats\\n',\n",
       " '\\tcoprophagies\\n',\n",
       " '\\tspriggers\\n',\n",
       " '\\tsplodges\\n',\n",
       " '\\tregimentals\\n',\n",
       " '\\twinterises\\n',\n",
       " '\\tsharedatas\\n',\n",
       " '\\tclaystones\\n',\n",
       " '\\tfloorshows\\n',\n",
       " '\\tcroups\\n',\n",
       " '\\tmodernizations\\n',\n",
       " '\\tliquidisers\\n',\n",
       " '\\trootlets\\n',\n",
       " '\\tclients\\n',\n",
       " '\\tviewfinders\\n',\n",
       " '\\ttanakhs\\n',\n",
       " '\\taplysias\\n',\n",
       " '\\ttreelesses\\n',\n",
       " '\\tshellbarks\\n',\n",
       " '\\tdisconsolates\\n',\n",
       " '\\tdalbergias\\n',\n",
       " '\\tanoestruss\\n',\n",
       " '\\tinductors\\n',\n",
       " '\\tmoils\\n',\n",
       " '\\tenologies\\n',\n",
       " '\\taxles\\n',\n",
       " '\\tanimalities\\n',\n",
       " '\\tsharers\\n',\n",
       " '\\tpyrographs\\n',\n",
       " '\\todysseuss\\n',\n",
       " '\\tchuddars\\n',\n",
       " '\\tsuborders\\n',\n",
       " '\\tmiroes\\n',\n",
       " '\\tstylizations\\n',\n",
       " '\\tsilverpoints\\n',\n",
       " '\\tezras\\n',\n",
       " '\\tantepenults\\n',\n",
       " '\\thaleys\\n',\n",
       " '\\tsauromaluss\\n',\n",
       " '\\tlasers\\n',\n",
       " '\\twheelhouses\\n',\n",
       " '\\tvirtuosities\\n',\n",
       " '\\tfundamentalisms\\n',\n",
       " '\\ttruisms\\n',\n",
       " '\\tcords\\n',\n",
       " '\\tcharlestowns\\n',\n",
       " '\\tbrookes\\n',\n",
       " '\\toverdramatizes\\n',\n",
       " '\\tpineals\\n',\n",
       " '\\tsubsisters\\n',\n",
       " '\\tisuis\\n',\n",
       " '\\tcroesuss\\n',\n",
       " '\\thephaestuss\\n',\n",
       " '\\tentices\\n',\n",
       " '\\tspikemosses\\n',\n",
       " '\\tarrants\\n',\n",
       " '\\tlomogrammas\\n',\n",
       " '\\tplaneras\\n',\n",
       " '\\tbowels\\n',\n",
       " '\\tcofferdams\\n',\n",
       " '\\tillegalities\\n',\n",
       " '\\tcalyculates\\n',\n",
       " '\\tdespotisms\\n',\n",
       " '\\tjungs\\n',\n",
       " '\\tdots\\n',\n",
       " '\\tenslavements\\n',\n",
       " '\\tadulteries\\n',\n",
       " '\\tquarantines\\n',\n",
       " '\\tcarditis\\n',\n",
       " '\\tjulyss\\n',\n",
       " '\\ttruths\\n',\n",
       " '\\tbaizas\\n',\n",
       " '\\tfaintings\\n',\n",
       " '\\tlms\\n',\n",
       " '\\tcounterdemonstrators\\n',\n",
       " '\\tcyberarts\\n',\n",
       " '\\tpyramids\\n',\n",
       " '\\tpennyweights\\n',\n",
       " '\\tadelies\\n',\n",
       " '\\tpollachiuss\\n',\n",
       " '\\tvoodoos\\n',\n",
       " '\\tmycoplasmataceaes\\n',\n",
       " '\\tnurtures\\n',\n",
       " '\\tpipeclays\\n',\n",
       " '\\tminicars\\n',\n",
       " '\\tdrops\\n',\n",
       " '\\tsmoothies\\n',\n",
       " '\\tsewards\\n',\n",
       " '\\tbounders\\n',\n",
       " '\\tpooches\\n',\n",
       " '\\troles\\n',\n",
       " '\\tbaseboards\\n',\n",
       " '\\taccommodations\\n',\n",
       " '\\tmeshugges\\n',\n",
       " '\\tfelicias\\n',\n",
       " '\\tperimeters\\n',\n",
       " '\\tmercerises\\n',\n",
       " '\\ttorreons\\n',\n",
       " '\\tcumbrias\\n',\n",
       " '\\tmockeries\\n',\n",
       " '\\tcartographers\\n',\n",
       " '\\thatcheries\\n',\n",
       " '\\tellipticities\\n',\n",
       " '\\tmyelocytes\\n',\n",
       " '\\tinspectorships\\n',\n",
       " '\\ttrusties\\n',\n",
       " '\\therbalists\\n',\n",
       " '\\tdraculas\\n',\n",
       " '\\tcoiffes\\n',\n",
       " '\\tlinocuts\\n',\n",
       " '\\tvideos\\n',\n",
       " '\\ttabours\\n',\n",
       " '\\treversals\\n',\n",
       " '\\talleghanies\\n',\n",
       " '\\tcoopers\\n',\n",
       " '\\tshamefulnesses\\n',\n",
       " '\\tepacridaceaes\\n',\n",
       " '\\teucalypts\\n',\n",
       " '\\tantepartums\\n',\n",
       " '\\trumors\\n',\n",
       " '\\tsensuousnesses\\n',\n",
       " '\\tcapitulations\\n',\n",
       " '\\tgrimnesses\\n',\n",
       " '\\tcircumscriptions\\n',\n",
       " '\\tkroneckers\\n',\n",
       " '\\tpeahens\\n',\n",
       " '\\tgeras\\n',\n",
       " '\\tinternalisations\\n',\n",
       " '\\tstiffeners\\n',\n",
       " '\\tasphodelines\\n',\n",
       " '\\tgessoes\\n',\n",
       " '\\tsignatories\\n',\n",
       " '\\tleftmosts\\n',\n",
       " '\\tappropriators\\n',\n",
       " '\\tclothes\\n',\n",
       " '\\ttetranychidaes\\n',\n",
       " '\\tmourners\\n',\n",
       " '\\ttearfuls\\n',\n",
       " '\\tmanglers\\n',\n",
       " '\\tthelarches\\n',\n",
       " '\\tcracklewares\\n',\n",
       " '\\tnis\\n',\n",
       " '\\tcanines\\n',\n",
       " '\\tacephalisms\\n',\n",
       " '\\tchockablocks\\n',\n",
       " '\\tcharcuteries\\n',\n",
       " '\\tmaillols\\n',\n",
       " '\\tamoebiases\\n',\n",
       " '\\ttomboys\\n',\n",
       " '\\tlapses\\n',\n",
       " '\\tdevilfish\\n',\n",
       " '\\tsashays\\n',\n",
       " '\\ttroublers\\n',\n",
       " '\\tporcelains\\n',\n",
       " '\\tstanzas\\n',\n",
       " '\\theadwords\\n',\n",
       " '\\tgages\\n',\n",
       " '\\ttrichodesmiums\\n',\n",
       " '\\tturnups\\n',\n",
       " '\\tserranids\\n',\n",
       " '\\tsanes\\n',\n",
       " '\\tquestions\\n',\n",
       " '\\tfoliations\\n',\n",
       " '\\tovercalls\\n',\n",
       " '\\tclusias\\n',\n",
       " '\\tanchormen\\n',\n",
       " '\\testablishmentarianisms\\n',\n",
       " '\\tsubduers\\n',\n",
       " '\\tpreexistents\\n',\n",
       " '\\tmillionaires\\n',\n",
       " '\\tmeshworks\\n',\n",
       " '\\turbanises\\n',\n",
       " '\\timmolations\\n',\n",
       " '\\tbernoullis\\n',\n",
       " '\\tdartmouths\\n',\n",
       " '\\tmcgs\\n',\n",
       " '\\tmilkcaps\\n',\n",
       " '\\themorrhages\\n',\n",
       " '\\turbanisations\\n',\n",
       " '\\tmonas\\n',\n",
       " '\\tchases\\n',\n",
       " '\\ttrichlormethiazides\\n',\n",
       " '\\tvientianes\\n',\n",
       " '\\ttumefactions\\n',\n",
       " '\\tpantheras\\n',\n",
       " '\\theterosexualities\\n',\n",
       " '\\ttitivations\\n',\n",
       " '\\twidgeons\\n',\n",
       " '\\tchanceries\\n',\n",
       " '\\tdanauss\\n',\n",
       " '\\tamrs\\n',\n",
       " '\\tfeet\\n',\n",
       " '\\texuberances\\n',\n",
       " '\\tosculates\\n',\n",
       " '\\tandroglossias\\n',\n",
       " '\\tspermatozoids\\n',\n",
       " '\\teretmochelyss\\n',\n",
       " '\\tballocks\\n',\n",
       " '\\tbutylates\\n',\n",
       " '\\tfootfalls\\n',\n",
       " '\\tshikokus\\n',\n",
       " '\\tkalkas\\n',\n",
       " '\\tholometabolies\\n',\n",
       " '\\tclavicipitaceaes\\n',\n",
       " '\\tmoneras\\n',\n",
       " '\\tprepositions\\n',\n",
       " '\\tmodems\\n',\n",
       " '\\tmyringotomies\\n',\n",
       " '\\tmaximizations\\n',\n",
       " '\\themolyses\\n',\n",
       " '\\tsudokus\\n',\n",
       " '\\topacities\\n',\n",
       " '\\tredundancies\\n',\n",
       " '\\tsuccessfulnesses\\n',\n",
       " '\\tanalyzers\\n',\n",
       " '\\tcankerworms\\n',\n",
       " '\\texenterations\\n',\n",
       " '\\tkohls\\n',\n",
       " '\\tbobbeds\\n',\n",
       " '\\tbrainies\\n',\n",
       " '\\thellenisms\\n',\n",
       " '\\tdisavows\\n',\n",
       " '\\tsemiconductors\\n',\n",
       " '\\thematocytopenias\\n',\n",
       " '\\tmaxillas\\n',\n",
       " '\\tlooneys\\n',\n",
       " '\\toverprices\\n',\n",
       " '\\tcumbersomes\\n',\n",
       " '\\tmaculas\\n',\n",
       " '\\thoecakes\\n',\n",
       " '\\tchiasmas\\n',\n",
       " '\\tmbabanes\\n',\n",
       " '\\tchildlikes\\n',\n",
       " '\\tsternnesses\\n',\n",
       " '\\tceftazidimes\\n',\n",
       " '\\tmonths\\n',\n",
       " '\\tcapitalists\\n',\n",
       " '\\tcomradelinesses\\n',\n",
       " '\\tpartialities\\n',\n",
       " '\\tlames\\n',\n",
       " '\\tnones\\n',\n",
       " '\\taccountants\\n',\n",
       " '\\tgenerosities\\n',\n",
       " '\\tays\\n',\n",
       " '\\texteriorizes\\n',\n",
       " '\\tstencils\\n',\n",
       " '\\tls\\n',\n",
       " '\\tailuropodidaes\\n',\n",
       " '\\tcephalochordates\\n',\n",
       " '\\tmultivalents\\n',\n",
       " '\\ttrentons\\n',\n",
       " '\\tmacrozamias\\n',\n",
       " '\\tbestows\\n',\n",
       " '\\tepicures\\n',\n",
       " '\\tzygophyllums\\n',\n",
       " '\\tcarboys\\n',\n",
       " '\\truffeds\\n',\n",
       " '\\tdispiritednesses\\n',\n",
       " '\\tignominiousnesses\\n',\n",
       " '\\tbourguignons\\n',\n",
       " '\\tfundraisings\\n',\n",
       " '\\tmercuries\\n',\n",
       " '\\tdysphorias\\n',\n",
       " '\\tsurrebutters\\n',\n",
       " '\\tfearlessnesses\\n',\n",
       " '\\treenlistments\\n',\n",
       " '\\ttollmen\\n',\n",
       " '\\tconvectors\\n',\n",
       " '\\tangwantiboes\\n',\n",
       " '\\tgruffs\\n',\n",
       " '\\tinvalidnesses\\n',\n",
       " '\\tdesiccates\\n',\n",
       " '\\tmods\\n',\n",
       " '\\tsaginaws\\n',\n",
       " '\\tconvallarias\\n',\n",
       " '\\tabstruses\\n',\n",
       " '\\tsusannas\\n',\n",
       " '\\tshopwalkers\\n',\n",
       " '\\thematocolposs\\n',\n",
       " '\\tcavallas\\n',\n",
       " '\\thabaneras\\n',\n",
       " '\\ttussahs\\n',\n",
       " '\\thermaphrodituss\\n',\n",
       " '\\tchampaigns\\n',\n",
       " '\\ttotaras\\n',\n",
       " '\\ttalapoins\\n',\n",
       " '\\tbreezelessnesses\\n',\n",
       " '\\tmemories\\n',\n",
       " '\\tfalanges\\n',\n",
       " '\\tvouges\\n',\n",
       " '\\tchemisorptions\\n',\n",
       " '\\twhiskeries\\n',\n",
       " '\\taltimeters\\n',\n",
       " '\\tunscrupulousnesses\\n',\n",
       " '\\tmalmoes\\n',\n",
       " '\\tresidents\\n',\n",
       " '\\tsaddlebills\\n',\n",
       " '\\tsmyrnas\\n',\n",
       " '\\troridulaceaes\\n',\n",
       " '\\tautisms\\n',\n",
       " '\\tthresholds\\n',\n",
       " '\\ttintinnabulations\\n',\n",
       " '\\tserbs\\n',\n",
       " '\\tcomplicities\\n',\n",
       " '\\tcoils\\n',\n",
       " '\\tchefs\\n',\n",
       " '\\tinsidiousnesses\\n',\n",
       " '\\tforums\\n',\n",
       " '\\tmizenmasts\\n',\n",
       " '\\tpresumes\\n',\n",
       " '\\ttopgallants\\n',\n",
       " '\\tconglobations\\n',\n",
       " '\\thalls\\n',\n",
       " '\\tgoldcups\\n',\n",
       " '\\tmotorises\\n',\n",
       " '\\tjonquils\\n',\n",
       " '\\tattackers\\n',\n",
       " '\\tmillwheels\\n',\n",
       " '\\tethenes\\n',\n",
       " '\\tleukocytes\\n',\n",
       " '\\tbjss\\n',\n",
       " '\\tawesomes\\n',\n",
       " '\\tnonwoodies\\n',\n",
       " '\\themosiderins\\n',\n",
       " '\\tlycaenas\\n',\n",
       " '\\tlandlesses\\n',\n",
       " '\\tabhors\\n',\n",
       " '\\tbrindisis\\n',\n",
       " '\\tsenatorships\\n',\n",
       " '\\tswietinias\\n',\n",
       " '\\tpianissimoes\\n',\n",
       " '\\tproducers\\n',\n",
       " '\\tneguss\\n',\n",
       " '\\tparallelopipedons\\n',\n",
       " '\\tcrosschecks\\n',\n",
       " '\\tsouthernisms\\n',\n",
       " '\\tsobrieties\\n',\n",
       " '\\tsurveillances\\n',\n",
       " '\\tthrottlers\\n',\n",
       " '\\tsanas\\n',\n",
       " '\\tpurines\\n',\n",
       " '\\theatheringtons\\n',\n",
       " '\\trobertsons\\n',\n",
       " '\\tethnologists\\n',\n",
       " '\\tchances\\n',\n",
       " '\\tprs\\n',\n",
       " '\\trumrunners\\n',\n",
       " '\\tdaimlers\\n',\n",
       " '\\tfeelers\\n',\n",
       " '\\tengelmannias\\n',\n",
       " '\\tstoops\\n',\n",
       " '\\tpapaveraceaes\\n',\n",
       " '\\tfalconers\\n',\n",
       " '\\tswordlikes\\n',\n",
       " '\\tretrovirs\\n',\n",
       " '\\tprigs\\n',\n",
       " '\\tlamps\\n',\n",
       " '\\tryas\\n',\n",
       " '\\tpsalmists\\n',\n",
       " '\\tcolligations\\n',\n",
       " '\\tinfirmities\\n',\n",
       " '\\tchoices\\n',\n",
       " '\\toos\\n',\n",
       " '\\tsleuthhounds\\n',\n",
       " '\\tconfidantes\\n',\n",
       " '\\thaliaeetuss\\n',\n",
       " '\\tshortgrasses\\n',\n",
       " '\\tscrapers\\n',\n",
       " '\\tmontenegroes\\n',\n",
       " '\\ttantalizes\\n',\n",
       " '\\tlances\\n',\n",
       " '\\tconfections\\n',\n",
       " '\\trecombines\\n',\n",
       " '\\tmythicises\\n',\n",
       " '\\tnicenesses\\n',\n",
       " '\\tacridnesses\\n',\n",
       " '\\tloquaciousnesses\\n',\n",
       " '\\txanthiums\\n',\n",
       " '\\tsymbolisations\\n',\n",
       " '\\tpetaloids\\n',\n",
       " '\\trepertoires\\n',\n",
       " '\\tbriefs\\n',\n",
       " '\\tturbidnesses\\n',\n",
       " '\\tmeuses\\n',\n",
       " '\\tsoupspoons\\n',\n",
       " '\\tliberations\\n',\n",
       " '\\textremists\\n',\n",
       " '\\tyons\\n',\n",
       " '\\teffs\\n',\n",
       " '\\twhaps\\n',\n",
       " '\\treconditenesses\\n',\n",
       " '\\tocelluss\\n',\n",
       " '\\tplanates\\n',\n",
       " '\\tactinozoans\\n',\n",
       " '\\tmates\\n',\n",
       " '\\tbromes\\n',\n",
       " '\\tcymas\\n',\n",
       " '\\tgroundmasses\\n',\n",
       " '\\tberteroas\\n',\n",
       " '\\tchinchillas\\n',\n",
       " '\\thydrofoils\\n',\n",
       " '\\tbirlings\\n',\n",
       " '\\tfounts\\n',\n",
       " '\\tmadronas\\n',\n",
       " '\\tchoragics\\n',\n",
       " '\\tsnickers\\n',\n",
       " '\\tdendromecons\\n',\n",
       " '\\tcockchafers\\n',\n",
       " '\\tscipios\\n',\n",
       " '\\tluminances\\n',\n",
       " '\\tadmeasurements\\n',\n",
       " '\\tsaddleries\\n',\n",
       " '\\trataplans\\n',\n",
       " '\\tupsurges\\n',\n",
       " '\\tsighs\\n',\n",
       " '\\tbroadleaves\\n',\n",
       " '\\tprimers\\n',\n",
       " '\\tbenthals\\n',\n",
       " '\\tsalvers\\n',\n",
       " '\\trockwells\\n',\n",
       " '\\tpanicles\\n',\n",
       " '\\tbambinoes\\n',\n",
       " '\\tbambuseaes\\n',\n",
       " '\\tilxes\\n',\n",
       " '\\ttessins\\n',\n",
       " '\\tsherwoods\\n',\n",
       " '\\tcachets\\n',\n",
       " '\\tpentoxides\\n',\n",
       " '\\teclecticists\\n',\n",
       " '\\tbrawns\\n',\n",
       " '\\tglossopsittas\\n',\n",
       " '\\tcutrers\\n',\n",
       " '\\tileostomies\\n',\n",
       " '\\tdocents\\n',\n",
       " '\\tjohannesburgs\\n',\n",
       " '\\tpointels\\n',\n",
       " '\\trnas\\n',\n",
       " '\\tbegones\\n',\n",
       " '\\tshoats\\n',\n",
       " '\\tphilogynies\\n',\n",
       " '\\tcdcs\\n',\n",
       " '\\tmows\\n',\n",
       " '\\tswatters\\n',\n",
       " '\\tmasrekahs\\n',\n",
       " '\\tdizens\\n',\n",
       " '\\taldermen\\n',\n",
       " '\\tforestries\\n',\n",
       " '\\tthuliums\\n',\n",
       " '\\tcomposes\\n',\n",
       " '\\tlimaxes\\n',\n",
       " '\\tloins\\n',\n",
       " '\\tanalphabets\\n',\n",
       " '\\tascensions\\n',\n",
       " '\\tclumsinesses\\n',\n",
       " '\\tgoldenseals\\n',\n",
       " '\\tnexus\\n',\n",
       " '\\tcubists\\n',\n",
       " '\\thumins\\n',\n",
       " '\\tfunduss\\n',\n",
       " '\\tspates\\n',\n",
       " '\\tdicta\\n',\n",
       " '\\tmsasas\\n',\n",
       " '\\tpediatrists\\n',\n",
       " '\\tethnographies\\n',\n",
       " '\\tranters\\n',\n",
       " '\\tregales\\n',\n",
       " '\\tprebons\\n',\n",
       " '\\tglechomas\\n',\n",
       " '\\tcollapses\\n',\n",
       " '\\tacariases\\n',\n",
       " '\\tabimeleches\\n',\n",
       " '\\tasangas\\n',\n",
       " '\\tchurls\\n',\n",
       " '\\tmangles\\n',\n",
       " '\\thumanities\\n',\n",
       " '\\tbearwoods\\n',\n",
       " '\\tdramatizations\\n',\n",
       " '\\talmoners\\n',\n",
       " '\\tmanilkaras\\n',\n",
       " '\\tcollectednesses\\n',\n",
       " '\\tpickles\\n',\n",
       " '\\tgilds\\n',\n",
       " '\\tbravenesses\\n',\n",
       " '\\tpretends\\n',\n",
       " '\\tmils\\n',\n",
       " '\\tmoneygrubbers\\n',\n",
       " '\\togresses\\n',\n",
       " '\\trapists\\n',\n",
       " '\\tstrenuosities\\n',\n",
       " '\\tincorruptnesses\\n',\n",
       " '\\tperignons\\n',\n",
       " '\\tpotomanias\\n',\n",
       " '\\trumbas\\n',\n",
       " '\\tshrines\\n',\n",
       " '\\tblurbs\\n',\n",
       " '\\tribands\\n',\n",
       " '\\tmerinoes\\n',\n",
       " '\\tindustriousnesses\\n',\n",
       " '\\tadvertents\\n',\n",
       " '\\tradiants\\n',\n",
       " '\\teuropeanisations\\n',\n",
       " '\\ttelecasts\\n',\n",
       " '\\tcadizs\\n',\n",
       " '\\tdads\\n',\n",
       " '\\tvulpeculas\\n',\n",
       " '\\tamplifies\\n',\n",
       " '\\tthwarts\\n',\n",
       " '\\tbiblesses\\n',\n",
       " '\\tbulgies\\n',\n",
       " '\\toutgoes\\n',\n",
       " '\\tpopularizations\\n',\n",
       " '\\tfischers\\n',\n",
       " '\\tstegocephalias\\n',\n",
       " '\\ttheophrastuss\\n',\n",
       " '\\tenlightenments\\n',\n",
       " '\\tfantails\\n',\n",
       " '\\tsontags\\n',\n",
       " '\\tinadvertencies\\n',\n",
       " '\\tdiscordants\\n',\n",
       " '\\themimetabolies\\n',\n",
       " '\\tthyrses\\n',\n",
       " '\\tfaraways\\n',\n",
       " '\\tczarinas\\n',\n",
       " '\\tintussusceptions\\n',\n",
       " '\\tverbatims\\n',\n",
       " '\\tpredestinarianisms\\n',\n",
       " '\\trecalcitrances\\n',\n",
       " '\\tspicules\\n',\n",
       " '\\taquilines\\n',\n",
       " '\\thomotherms\\n',\n",
       " '\\taeolotropics\\n',\n",
       " '\\thayrigs\\n',\n",
       " '\\tbriars\\n',\n",
       " '\\ttherewiths\\n',\n",
       " '\\tesfahans\\n',\n",
       " '\\tmadderworts\\n',\n",
       " '\\tmultivalencies\\n',\n",
       " '\\tsaurias\\n',\n",
       " '\\tcontinues\\n',\n",
       " '\\tlaffers\\n',\n",
       " '\\tunderbids\\n',\n",
       " '\\tanthozoas\\n',\n",
       " '\\tcrampfish\\n',\n",
       " '\\tinitiators\\n',\n",
       " '\\tanachronisms\\n',\n",
       " '\\tpsittacuss\\n',\n",
       " '\\theaps\\n',\n",
       " '\\tprecipitousnesses\\n',\n",
       " '\\tukes\\n',\n",
       " '\\tcalcuttans\\n',\n",
       " '\\tlactases\\n',\n",
       " '\\ttimeworks\\n',\n",
       " '\\ttaxabilities\\n',\n",
       " '\\tdardanuss\\n',\n",
       " '\\tliriopes\\n',\n",
       " '\\tdisapprobations\\n',\n",
       " '\\tglamorises\\n',\n",
       " '\\tnrtis\\n',\n",
       " '\\tberries\\n',\n",
       " '\\tpatentees\\n',\n",
       " '\\tsecondments\\n',\n",
       " '\\tstipends\\n',\n",
       " '\\tmesquites\\n',\n",
       " '\\tconsuls\\n',\n",
       " '\\tcyons\\n',\n",
       " '\\tscoffers\\n',\n",
       " '\\thumours\\n',\n",
       " '\\tscalenuss\\n',\n",
       " '\\tgywns\\n',\n",
       " '\\tlankies\\n',\n",
       " '\\tairmanships\\n',\n",
       " '\\tradiophotographies\\n',\n",
       " '\\tmelancholiacs\\n',\n",
       " '\\tpromiscuities\\n',\n",
       " '\\thermissendas\\n',\n",
       " '\\temirs\\n',\n",
       " '\\tmujahidins\\n',\n",
       " '\\tgeothlypiss\\n',\n",
       " '\\tbattlewagons\\n',\n",
       " '\\tjoylessnesses\\n',\n",
       " '\\tinternationalizations\\n',\n",
       " '\\thusains\\n',\n",
       " '\\tanalysts\\n',\n",
       " '\\teolithics\\n',\n",
       " '\\tvis\\n',\n",
       " '\\tcabalisms\\n',\n",
       " '\\treticences\\n',\n",
       " '\\tsputums\\n',\n",
       " '\\texpeditiousnesses\\n',\n",
       " '\\tjellyrolls\\n',\n",
       " '\\tyes\\n',\n",
       " '\\tbountifulnesses\\n',\n",
       " '\\thydnocarpuss\\n',\n",
       " '\\tsuperciliousnesses\\n',\n",
       " '\\tcompliances\\n',\n",
       " '\\tinexperiences\\n',\n",
       " '\\tamelioratories\\n',\n",
       " '\\tmicronesias\\n',\n",
       " '\\tderriss\\n',\n",
       " '\\tpyroxylines\\n',\n",
       " '\\tloyalties\\n',\n",
       " '\\tscouts\\n',\n",
       " '\\tprofligacies\\n',\n",
       " '\\traisers\\n',\n",
       " '\\tsweatbands\\n',\n",
       " '\\tspodes\\n',\n",
       " '\\timaums\\n',\n",
       " '\\tligules\\n',\n",
       " '\\tzones\\n',\n",
       " '\\tembankments\\n',\n",
       " '\\tclerestories\\n',\n",
       " '\\tcoordinators\\n',\n",
       " '\\tinadequacies\\n',\n",
       " '\\tfuzees\\n',\n",
       " '\\twagerers\\n',\n",
       " '\\telspars\\n',\n",
       " '\\tscollops\\n',\n",
       " '\\tplethodons\\n',\n",
       " '\\ttones\\n',\n",
       " '\\tmacrencephalies\\n',\n",
       " '\\tinvariants\\n',\n",
       " '\\tvendomes\\n',\n",
       " '\\tneuroepitheliomas\\n',\n",
       " '\\tipods\\n',\n",
       " '\\tarchidiskidons\\n',\n",
       " '\\tprises\\n',\n",
       " '\\tchalkstones\\n',\n",
       " '\\tanophelines\\n',\n",
       " '\\tmarlinspikes\\n',\n",
       " '\\tperons\\n',\n",
       " '\\tfoolings\\n',\n",
       " '\\tcockups\\n',\n",
       " '\\ttoads\\n',\n",
       " '\\tpelargoniums\\n',\n",
       " '\\tastrologists\\n',\n",
       " '\\tponycarts\\n',\n",
       " '\\tdenigrates\\n',\n",
       " '\\tcompactions\\n',\n",
       " '\\tmanias\\n',\n",
       " '\\twesterners\\n',\n",
       " '\\tbarbiturates\\n',\n",
       " '\\tgraduates\\n',\n",
       " '\\tlucanidaes\\n',\n",
       " '\\tmukataas\\n',\n",
       " '\\tleningrads\\n',\n",
       " '\\tmiddles\\n',\n",
       " '\\tmaplelikes\\n',\n",
       " '\\tbassoonists\\n',\n",
       " '\\temploys\\n',\n",
       " '\\teffendis\\n',\n",
       " '\\texclusions\\n',\n",
       " '\\tprouders\\n',\n",
       " '\\tboggies\\n',\n",
       " '\\tkoizumis\\n',\n",
       " '\\tdecolonizations\\n',\n",
       " '\\tvexes\\n',\n",
       " '\\tvaccines\\n',\n",
       " '\\troentgeniums\\n',\n",
       " '\\tchristlikes\\n',\n",
       " '\\tatopognosias\\n',\n",
       " '\\tpelobatidaes\\n',\n",
       " '\\tbranchias\\n',\n",
       " '\\tslylies\\n',\n",
       " '\\tdevelopments\\n',\n",
       " '\\tbayers\\n',\n",
       " '\\tdicranums\\n',\n",
       " '\\taggressions\\n',\n",
       " '\\tglucosides\\n',\n",
       " '\\tcyclosoruss\\n',\n",
       " '\\tsnakes\\n',\n",
       " '\\tsandglasses\\n',\n",
       " '\\tmeddlers\\n',\n",
       " '\\tceilings\\n',\n",
       " '\\tunambiguities\\n',\n",
       " '\\tsqualidaes\\n',\n",
       " '\\tculms\\n',\n",
       " '\\taltitudes\\n',\n",
       " '\\treservoirs\\n',\n",
       " '\\tscalers\\n',\n",
       " '\\theterologics\\n',\n",
       " '\\tarousers\\n',\n",
       " '\\tabeams\\n',\n",
       " '\\tchlamydospores\\n',\n",
       " '\\twretchednesses\\n',\n",
       " '\\thierolatries\\n',\n",
       " '\\thaftarahs\\n',\n",
       " '\\tgirondins\\n',\n",
       " '\\ttransporters\\n',\n",
       " '\\tobjects\\n',\n",
       " '\\tcriminatories\\n',\n",
       " '\\tmusquashes\\n',\n",
       " '\\tvasoconstrictions\\n',\n",
       " '\\tbenoes\\n',\n",
       " '\\tinstitutions\\n',\n",
       " '\\teigens\\n',\n",
       " '\\ttaxonomers\\n',\n",
       " '\\tstepsisters\\n',\n",
       " '\\tpneumovaxes\\n',\n",
       " '\\tfrissons\\n',\n",
       " '\\tphenols\\n',\n",
       " '\\tcatawbas\\n',\n",
       " '\\tterras\\n',\n",
       " '\\tloaves\\n',\n",
       " '\\teds\\n',\n",
       " '\\tecologies\\n',\n",
       " '\\ttrichotomies\\n',\n",
       " '\\tsticks\\n',\n",
       " '\\tamongsts\\n',\n",
       " '\\themogeneses\\n',\n",
       " '\\tdisconnectednesses\\n',\n",
       " '\\tserviettes\\n',\n",
       " '\\tauctions\\n',\n",
       " '\\tsocializes\\n',\n",
       " '\\tsarracenias\\n',\n",
       " '\\tpoes\\n',\n",
       " '\\tiliads\\n',\n",
       " '\\ttransitivenesses\\n',\n",
       " '\\tgladiators\\n',\n",
       " '\\tluxes\\n',\n",
       " '\\ttamarins\\n',\n",
       " '\\tkrasners\\n',\n",
       " '\\tmaidenhoods\\n',\n",
       " '\\tbotflies\\n',\n",
       " '\\tjiggles\\n',\n",
       " '\\tjerevans\\n',\n",
       " '\\tanestrums\\n',\n",
       " '\\tbibliopoles\\n',\n",
       " '\\tincloses\\n',\n",
       " '\\tredberries\\n',\n",
       " '\\tmansons\\n',\n",
       " '\\tsinks\\n',\n",
       " '\\tswerves\\n',\n",
       " '\\treceives\\n',\n",
       " '\\tpungs\\n',\n",
       " '\\tsamples\\n',\n",
       " '\\tidentifies\\n',\n",
       " '\\tbedews\\n',\n",
       " '\\tvapouries\\n',\n",
       " '\\tmallees\\n',\n",
       " '\\tgawkies\\n',\n",
       " '\\tcastilleias\\n',\n",
       " '\\tquickies\\n',\n",
       " '\\tticktacktoes\\n',\n",
       " '\\tcornerbacks\\n',\n",
       " '\\tfemurs\\n',\n",
       " '\\tcalostomataceaes\\n',\n",
       " '\\tglottochronologies\\n',\n",
       " '\\teruptions\\n',\n",
       " '\\tcowmen\\n',\n",
       " '\\tinvestigations\\n',\n",
       " '\\tgreybeards\\n',\n",
       " '\\tviewerships\\n',\n",
       " '\\tcercidiums\\n',\n",
       " '\\tcookeries\\n',\n",
       " '\\tincubations\\n',\n",
       " '\\tpetrologies\\n',\n",
       " '\\tbunks\\n',\n",
       " '\\trestlesses\\n',\n",
       " '\\tidealizations\\n',\n",
       " '\\tamericiums\\n',\n",
       " '\\tsootinesses\\n',\n",
       " '\\tmelancholies\\n',\n",
       " '\\tcjds\\n',\n",
       " '\\ttacklers\\n',\n",
       " '\\tcandidacies\\n',\n",
       " '\\tliteralisms\\n',\n",
       " '\\tshipways\\n',\n",
       " '\\tazadirachtins\\n',\n",
       " '\\tmahalaleels\\n',\n",
       " '\\tpraters\\n',\n",
       " '\\tcopepods\\n',\n",
       " '\\tpinaceaes\\n',\n",
       " '\\toffers\\n',\n",
       " '\\tcoerebidaes\\n',\n",
       " '\\tfads\\n',\n",
       " '\\tmidveins\\n',\n",
       " '\\televations\\n',\n",
       " '\\tgabardines\\n',\n",
       " '\\tmorrows\\n',\n",
       " '\\tintensivenesses\\n',\n",
       " '\\thiplengths\\n',\n",
       " '\\tfoetids\\n',\n",
       " '\\tcorpulents\\n',\n",
       " '\\tlycees\\n',\n",
       " '\\tdryass\\n',\n",
       " '\\tspironolactones\\n',\n",
       " '\\tdirectories\\n',\n",
       " '\\tvesiculitis\\n',\n",
       " '\\tcrazes\\n',\n",
       " '\\tglazers\\n',\n",
       " '\\tmyelitis\\n',\n",
       " '\\tzealots\\n',\n",
       " '\\tbegrudges\\n',\n",
       " '\\tgallants\\n',\n",
       " '\\tpetechias\\n',\n",
       " '\\tbeneficiates\\n',\n",
       " '\\ttheologisers\\n',\n",
       " '\\tadvantageousnesses\\n',\n",
       " '\\tgambits\\n',\n",
       " '\\treferendums\\n',\n",
       " '\\ttuberaceaes\\n',\n",
       " '\\tangiitis\\n',\n",
       " '\\talbizzias\\n',\n",
       " '\\tincompetents\\n',\n",
       " '\\tcoaxals\\n',\n",
       " '\\tevermores\\n',\n",
       " '\\tcollaterals\\n',\n",
       " '\\tperiapses\\n',\n",
       " '\\tratholes\\n',\n",
       " '\\traskolnikovs\\n',\n",
       " '\\truthlessnesses\\n',\n",
       " '\\tislamabads\\n',\n",
       " '\\twoodcrafts\\n',\n",
       " '\\tdeadheads\\n',\n",
       " '\\tcreatures\\n',\n",
       " '\\tspizellas\\n',\n",
       " '\\tglycosides\\n',\n",
       " '\\tbractlets\\n',\n",
       " '\\thaastias\\n',\n",
       " '\\tmansarts\\n',\n",
       " '\\tratios\\n',\n",
       " '\\ttamars\\n',\n",
       " '\\tangiologies\\n',\n",
       " '\\tcrafts\\n',\n",
       " '\\tsimplicities\\n',\n",
       " '\\tyeshivahs\\n',\n",
       " '\\trauls\\n',\n",
       " '\\toys\\n',\n",
       " '\\ttechnophobes\\n',\n",
       " '\\tfeathers\\n',\n",
       " '\\tglutamines\\n',\n",
       " '\\tformlesses\\n',\n",
       " '\\tkindergartners\\n',\n",
       " '\\ttamps\\n',\n",
       " '\\tcentilitres\\n',\n",
       " '\\tarchaises\\n',\n",
       " '\\texcesses\\n',\n",
       " '\\tmetersticks\\n',\n",
       " '\\toligarches\\n',\n",
       " '\\tbovids\\n',\n",
       " '\\tvalues\\n',\n",
       " '\\tbiofeedbacks\\n',\n",
       " '\\tlicentiousnesses\\n',\n",
       " '\\tsandlikes\\n',\n",
       " '\\tdichroisms\\n',\n",
       " '\\tvithars\\n',\n",
       " '\\taccurses\\n',\n",
       " '\\tcatboats\\n',\n",
       " '\\tserins\\n',\n",
       " '\\tconcomitants\\n',\n",
       " '\\tfertilizations\\n',\n",
       " '\\ttramps\\n',\n",
       " '\\tpeneidaes\\n',\n",
       " '\\tinvigilates\\n',\n",
       " '\\tprimalities\\n',\n",
       " '\\tprolepses\\n',\n",
       " '\\tcervicitis\\n',\n",
       " '\\tmithramycins\\n',\n",
       " '\\thelminths\\n',\n",
       " '\\tchunnels\\n',\n",
       " '\\tarcanes\\n',\n",
       " '\\tdessiatines\\n',\n",
       " '\\tprepossesses\\n',\n",
       " '\\taides\\n',\n",
       " '\\tborshches\\n',\n",
       " '\\tatonalities\\n',\n",
       " '\\tchechens\\n',\n",
       " '\\ttomfooleries\\n',\n",
       " '\\tboomers\\n',\n",
       " '\\thypermenorrheas\\n',\n",
       " '\\tsuckers\\n',\n",
       " '\\tchandis\\n',\n",
       " '\\tsravanas\\n',\n",
       " '\\treforestations\\n',\n",
       " '\\tpaleornithologies\\n',\n",
       " '\\telbows\\n',\n",
       " '\\tslipperworts\\n',\n",
       " '\\tbaryes\\n',\n",
       " '\\tromanias\\n',\n",
       " '\\thypenteliums\\n',\n",
       " '\\tpaleomammalogies\\n',\n",
       " '\\tpalates\\n',\n",
       " '\\tconcurrences\\n',\n",
       " '\\tfistulinas\\n',\n",
       " '\\tciscoes\\n',\n",
       " '\\tmalarkies\\n',\n",
       " '\\ttsars\\n',\n",
       " '\\tdisorganizations\\n',\n",
       " '\\tgendarmes\\n',\n",
       " '\\twizs\\n',\n",
       " '\\tprovabilities\\n',\n",
       " '\\tamortisations\\n',\n",
       " '\\tfederates\\n',\n",
       " '\\tcossets\\n',\n",
       " '\\tcoerces\\n',\n",
       " '\\tcharcots\\n',\n",
       " '\\tnagoyas\\n',\n",
       " '\\tzeemen\\n',\n",
       " '\\tbrows\\n',\n",
       " '\\teucharists\\n',\n",
       " '\\tenteroptoses\\n',\n",
       " '\\tquiescences\\n',\n",
       " '\\tcraters\\n',\n",
       " '\\tcalabas\\n',\n",
       " '\\tinnkeepers\\n',\n",
       " '\\tabacuss\\n',\n",
       " '\\tmidgrasses\\n',\n",
       " '\\tstirs\\n',\n",
       " '\\teminences\\n',\n",
       " '\\tpantries\\n',\n",
       " '\\tcampaigners\\n',\n",
       " '\\tdopas\\n',\n",
       " '\\thasts\\n',\n",
       " '\\ttamandus\\n',\n",
       " '\\twanderlusts\\n',\n",
       " '\\tcatechus\\n',\n",
       " '\\tlarns\\n',\n",
       " '\\tsouaris\\n',\n",
       " '\\tshtikls\\n',\n",
       " '\\taggressivenesses\\n',\n",
       " '\\tstenographies\\n',\n",
       " '\\tfiretraps\\n',\n",
       " '\\tmajorcas\\n',\n",
       " '\\taestivals\\n',\n",
       " '\\tdebitors\\n',\n",
       " '\\tbitters\\n',\n",
       " '\\tchronoscopes\\n',\n",
       " '\\ttradescantias\\n',\n",
       " '\\tgunites\\n',\n",
       " '\\tabolishments\\n',\n",
       " '\\tsemisweets\\n',\n",
       " '\\tsorrows\\n',\n",
       " '\\tmalleabilities\\n',\n",
       " '\\tfarmyards\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 3500\n",
    "s_words = s_words[0:total]\n",
    "es_words.extend(es_words[0:total - len(es_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Augmenting ies words as a way of weighted sampling\n",
    "ies_words.extend(ies_words[0:total - len(ies_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "others = others[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of s_words:  3500\n",
      "Number of es_words:  3500\n",
      "Number of ies_words:  3500\n",
      "Number of other_words:  500\n",
      "Total words:  11000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of s_words: \", len(s_words))\n",
    "print(\"Number of es_words: \", len(es_words))\n",
    "print(\"Number of ies_words: \", len(ies_words))\n",
    "print(\"Number of other_words: \", len(others))\n",
    "print(\"Total words: \", len(s_words) + len(es_words) + len(ies_words) + len(others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_length = 3150\n",
    "train_indices = s_words[0:train_length] + es_words[0:train_length] + ies_words[0:train_length] + others[0:450]\n",
    "test_indices = s_words[train_length:] + es_words[train_length:] + ies_words[train_length:] + others[450:]\n",
    "random.seed(1)\n",
    "random.shuffle(train_indices)\n",
    "random.shuffle(test_indices)\n",
    "\n",
    "train_input = [input_texts[i] for i in train_indices]\n",
    "train_output = [target_texts[i] for i in train_indices]\n",
    "\n",
    "test_input = [input_texts[i] for i in test_indices]\n",
    "test_output = [target_texts[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system(\"rm train.csv\")\n",
    "with open('train.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    for i in range(len(train_input)):\n",
    "#         print(\"Input: \", train_input[i], \"Output: \", train_output[i])\n",
    "        writer.writerow([train_input[i], train_output[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system(\"rm test.csv\")\n",
    "with open('test.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    for i in range(len(test_input)):\n",
    "#         print(\"Input: \", test_input[i], \"Output: \", test_output[i])\n",
    "        writer.writerow([test_input[i], test_output[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = []\n",
    "train_output = []\n",
    "with open('train_ending.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        train_input.append(row[0])\n",
    "        train_output.append(row[1]) # replace with row[1][1:-1] if you don't want \\t and \\n around the word\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = []\n",
    "test_output = []\n",
    "with open('test_ending.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        test_input.append(row[0])\n",
    "        test_output.append(row[1]) # replace with row[1][1:-1] if you don't want \\t and \\n around the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of s_words:  3150\n",
      "Number of es_words:  3150\n",
      "Number of ies_words:  3150\n",
      "Number of other_words:  450\n",
      "Total words:  9900\n"
     ]
    }
   ],
   "source": [
    "# Check for the count in train dataset\n",
    "s_words = []\n",
    "es_words = []\n",
    "ies_words = []\n",
    "others = []\n",
    "for i in range(len(train_input)):\n",
    "    input_sample = train_input[i]\n",
    "    output_sample = train_output[i][1:-1]\n",
    "\n",
    "    if (input_sample[-2:] == 'ch' or input_sample[-2:] == 'sh' or input_sample[-1:] == 'x' or \\\n",
    "        input_sample[-1:] == 's' or input_sample[-1:] == 'z') and output_sample[-2:] == 'es':\n",
    "        es_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\tes\\n'\n",
    "    elif input_sample[-1:] == 'y' and output_sample[-3:] == 'ies':\n",
    "        ies_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ties\\n'\n",
    "    elif output_sample[-1:] == 's':\n",
    "        s_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ts\\n'\n",
    "    else:\n",
    "        others.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\t\\n'\n",
    "\n",
    "print(\"Number of s_words: \", len(s_words))\n",
    "print(\"Number of es_words: \", len(es_words))\n",
    "print(\"Number of ies_words: \", len(ies_words))\n",
    "print(\"Number of other_words: \", len(others))\n",
    "print(\"Total words: \", len(s_words) + len(es_words) + len(ies_words) + len(others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of s_words:  350\n",
      "Number of es_words:  350\n",
      "Number of ies_words:  350\n",
      "Number of other_words:  50\n",
      "Total words:  1100\n"
     ]
    }
   ],
   "source": [
    "# Check for the count in train dataset\n",
    "s_words = []\n",
    "es_words = []\n",
    "ies_words = []\n",
    "others = []\n",
    "for i in range(len(test_input)):\n",
    "    input_sample = test_input[i]\n",
    "    output_sample = test_output[i][1:-1]\n",
    "\n",
    "    if (input_sample[-2:] == 'ch' or input_sample[-2:] == 'sh' or input_sample[-1:] == 'x' or \\\n",
    "        input_sample[-1:] == 's' or input_sample[-1:] == 'z') and output_sample[-2:] == 'es':\n",
    "        es_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\tes\\n'\n",
    "    elif input_sample[-1:] == 'y' and output_sample[-3:] == 'ies':\n",
    "        ies_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ties\\n'\n",
    "    elif output_sample[-1:] == 's':\n",
    "        s_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ts\\n'\n",
    "    else:\n",
    "        others.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\t\\n'\n",
    "\n",
    "print(\"Number of s_words: \", len(s_words))\n",
    "print(\"Number of es_words: \", len(es_words))\n",
    "print(\"Number of ies_words: \", len(ies_words))\n",
    "print(\"Number of other_words: \", len(others))\n",
    "print(\"Total words: \", len(s_words) + len(es_words) + len(ies_words) + len(others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of s_words:  350\n",
      "Number of es_words:  350\n",
      "Number of ies_words:  350\n",
      "Number of other_words:  50\n",
      "Total words:  1100\n"
     ]
    }
   ],
   "source": [
    "# Check for the count in test dataset\n",
    "s_words = []\n",
    "es_words = []\n",
    "ies_words = []\n",
    "others = []\n",
    "for i in range(len(test_input)):\n",
    "    input_sample = test_input[i]\n",
    "    output_sample = test_output[i][1:-1]\n",
    "\n",
    "    if (input_sample[-2:] == 'ch' or input_sample[-2:] == 'sh' or input_sample[-1:] == 'x' or \\\n",
    "        input_sample[-1:] == 's' or input_sample[-1:] == 'z') and output_sample[-2:] == 'es':\n",
    "        es_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\tes\\n'\n",
    "    elif input_sample[-1:] == 'y' and output_sample[-3:] == 'ies':\n",
    "        ies_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ties\\n'\n",
    "    elif output_sample[-1:] == 's':\n",
    "        s_words.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\ts\\n'\n",
    "    else:\n",
    "        others.append(i)\n",
    "        if classification_flag:\n",
    "            target_texts[i] = '\\t\\n'\n",
    "\n",
    "print(\"Number of s_words: \", len(s_words))\n",
    "print(\"Number of es_words: \", len(es_words))\n",
    "print(\"Number of ies_words: \", len(ies_words))\n",
    "print(\"Number of other_words: \", len(others))\n",
    "print(\"Total words: \", len(s_words) + len(es_words) + len(ies_words) + len(others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set input:  9900\n",
      "Number of samples in training set output:  9900\n",
      "Number of samples in testing set input:  1100\n",
      "Number of samples in testing set output:  1100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in training set input: \", len(train_input))\n",
    "print(\"Number of samples in training set output: \", len(train_output))\n",
    "print(\"Number of samples in testing set input: \", len(test_input))\n",
    "print(\"Number of samples in testing set output: \", len(test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts = train_input + test_input\n",
    "target_texts = train_output + test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 11000\n",
      "Number of unique input tokens: 26\n",
      "Number of unique output tokens: 28\n",
      "Max sequence length for inputs: 31\n",
      "Max sequence length for outputs: 5\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "#     print(\"Inside decode_sequence: \", states_value[0].shape, states_value[1].shape)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence, states_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_encoder_input_data = encoder_input_data[0:len(train_input)]\n",
    "train_decoder_input_data = decoder_input_data[0:len(train_input)]\n",
    "train_decoder_target_data = decoder_target_data[0:len(train_input)]\n",
    "test_encoder_input_data = encoder_input_data[len(train_input):]\n",
    "test_decoder_input_data = decoder_input_data[len(train_input):]\n",
    "test_decoder_target_data = decoder_target_data[len(train_input):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for it in range(1, 11):\n",
    "# latent_dim = it * 5\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.load_weights('./final_models/rmsprop_model_500_' + str(latent_dim) + '.h5')\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "print(str(datetime.datetime.now()), \"Training with \", latent_dim,\n",
    "      \" latent dimensions\")\n",
    "\n",
    "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.0,\n",
    "          verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save('./final_models/rmsprop_model_500_'+str(latent_dim)+'.h5')\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "delta = 0\n",
    "correct = 0\n",
    "for seq_index in range(len(test_input)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying outdecoding.\n",
    "    if seq_index % 100 == 0:\n",
    "        print(seq_index, str(datetime.datetime.now()), correct)\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index+ 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq)\n",
    "    if decoded_sentence == test_output[seq_index][1:]:\n",
    "        correct += 1\n",
    "    \n",
    "#     print('Singular:', test_input[seq_index])\n",
    "#     print('Plural:', decoded_sentence)\n",
    "#     print('Target:', test_output[seq_index])\n",
    "\n",
    "print(\"Latent Dim: \", latent_dim, correct/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-05-13 19:27:48.450690 0\n",
      "100 2018-05-13 19:27:49.816087 96\n",
      "200 2018-05-13 19:27:51.171339 192\n",
      "300 2018-05-13 19:27:52.525184 287\n",
      "400 2018-05-13 19:27:53.910028 382\n",
      "500 2018-05-13 19:27:55.243190 477\n",
      "600 2018-05-13 19:27:56.585720 569\n",
      "700 2018-05-13 19:27:57.972080 659\n",
      "800 2018-05-13 19:27:59.345989 755\n",
      "900 2018-05-13 19:28:00.690084 852\n",
      "1000 2018-05-13 19:28:02.028956 947\n",
      "Latent Dim:  10 0.9481818181818182\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14 22:14:18.756248 Training with  5  latent dimensions\n",
      "maachah \ts\n",
      "\n",
      "research \t\n",
      "\n",
      "lymphoma \t\n",
      "\n",
      "kildeer \t\n",
      "\n",
      "counterperson \t\n",
      "\n",
      "acousma \ts\n",
      "\n",
      "jaffa \ts\n",
      "\n",
      "delouse \t\n",
      "\n",
      "meat \t\n",
      "\n",
      "spokesperson \t\n",
      "\n",
      "basso \t\n",
      "\n",
      "cheesy \ties\n",
      "\n",
      "drama \t\n",
      "\n",
      "lamedh \ts\n",
      "\n",
      "memorandum \t\n",
      "\n",
      "splayfoot \t\n",
      "\n",
      "radius \t\n",
      "\n",
      "pox \t\n",
      "\n",
      "bread \t\n",
      "\n",
      "entozoon \t\n",
      "\n",
      "lagopus \t\n",
      "\n",
      "phalaropus \t\n",
      "\n",
      "manawydan \ts\n",
      "\n",
      "child \t\n",
      "\n",
      "rhinoceros \ts\n",
      "\n",
      "schema \t\n",
      "\n",
      "happiness \ts\n",
      "\n",
      "furniture \t\n",
      "\n",
      "cow \t\n",
      "\n",
      "flounder \t\n",
      "\n",
      "sabertooth \t\n",
      "\n",
      "sheep \t\n",
      "\n",
      "pronoun \ts\n",
      "\n",
      "genus \t\n",
      "\n",
      "mustard \t\n",
      "\n",
      "corpus \t\n",
      "\n",
      "caribou \ts\n",
      "\n",
      "bacterium \t\n",
      "\n",
      "love \t\n",
      "\n",
      "contopus \t\n",
      "\n",
      "lieu \t\n",
      "\n",
      "candelabrum \t\n",
      "\n",
      "ganglion \t\n",
      "\n",
      "soupspoon \ts\n",
      "\n",
      "trichodesmium \ts\n",
      "\n",
      "formula \t\n",
      "\n",
      "perihelion \t\n",
      "\n",
      "himantopus \t\n",
      "\n",
      "malarkey \ts\n",
      "\n",
      "leucocytozoon \t\n",
      "\n",
      "pyrocellulose \t\n",
      "\n",
      "equipment \t\n",
      "\n",
      "woodlouse \t\n",
      "\n",
      "javanthropus \t\n",
      "\n",
      "centropus \t\n",
      "\n",
      "bison \t\n",
      "\n",
      "newsperson \t\n",
      "\n",
      "tenderfoot \t\n",
      "\n",
      "mayonnaise \t\n",
      "\n",
      "analyst \ts\n",
      "\n",
      "advice \t\n",
      "\n",
      "crampfish \t\n",
      "\n",
      "blackfoot \t\n",
      "\n",
      "cowpox \t\n",
      "\n",
      "money \ties\n",
      "\n",
      "rostrum \t\n",
      "\n",
      "arachis \ts\n",
      "\n",
      "xenopus \t\n",
      "\n",
      "pseudechis \ts\n",
      "\n",
      "chisinau \ts\n",
      "\n",
      "lomogramma \ts\n",
      "\n",
      "water \t\n",
      "\n",
      "hyssopus \t\n",
      "\n",
      "phylum \t\n",
      "\n",
      "consortium \t\n",
      "\n",
      "panfish \t\n",
      "\n",
      "spermatozoon \t\n",
      "\n",
      "kadikoy \ts\n",
      "\n",
      "aquarium \t\n",
      "\n",
      "electricity \t\n",
      "\n",
      "edema \t\n",
      "\n",
      "briefs \ts\n",
      "\n",
      "sydney \ts\n",
      "\n",
      "chauffeuse \ts\n",
      "\n",
      "medium \t\n",
      "\n",
      "lycopus \t\n",
      "\n",
      "person \t\n",
      "\n",
      "blouse \t\n",
      "\n",
      "oman \t\n",
      "\n",
      "soprano \t\n",
      "\n",
      "devilfish \t\n",
      "\n",
      "zinjanthropus \t\n",
      "\n",
      "macropus \t\n",
      "\n",
      "genius \t\n",
      "\n",
      "ectozoon \t\n",
      "\n",
      "fungus \t\n",
      "\n",
      "news \ts\n",
      "\n",
      "rickettsialpox \t\n",
      "\n",
      "fruit \t\n",
      "\n",
      "alto \t\n",
      "\n",
      "crescendo \t\n",
      "\n",
      "persimmon \ts\n",
      "\n",
      "curriculum \t\n",
      "\n",
      "mokulu \ts\n",
      "\n",
      "mouse \t\n",
      "\n",
      "honorarium \t\n",
      "\n",
      "chairperson \t\n",
      "\n",
      "interregnum \t\n",
      "\n",
      "ox \t\n",
      "\n",
      "y \ties\n",
      "\n",
      "tussah \ts\n",
      "\n",
      "protozoon \t\n",
      "\n",
      "pinmoney \ties\n",
      "\n",
      "booby \ties\n",
      "\n",
      "pseudosmallpox \t\n",
      "\n",
      "montesquieu \t\n",
      "\n",
      "businessperson \t\n",
      "\n",
      "foot \t\n",
      "\n",
      "seraph \t\n",
      "\n",
      "ketchup \t\n",
      "\n",
      "information \t\n",
      "\n",
      "mooo \ts\n",
      "\n",
      "maffia \ts\n",
      "\n",
      "goy \t\n",
      "\n",
      "gravel \t\n",
      "\n",
      "understanding \t\n",
      "\n",
      "himmalehs \ts\n",
      "\n",
      "liveth \ts\n",
      "\n",
      "tuna \t\n",
      "\n",
      "criterion \t\n",
      "\n",
      "belau \ts\n",
      "\n",
      "nonperson \t\n",
      "\n",
      "ptychozoon \t\n",
      "\n",
      "diploma \t\n",
      "\n",
      "emporium \t\n",
      "\n",
      "sigh \ts\n",
      "\n",
      "homework \t\n",
      "\n",
      "soma \t\n",
      "\n",
      "stratum \t\n",
      "\n",
      "torus \t\n",
      "\n",
      "swbs \ts\n",
      "\n",
      "salmon \t\n",
      "\n",
      "yeshivah \ts\n",
      "\n",
      "anathema \t\n",
      "\n",
      "orycteropus \t\n",
      "\n",
      "tableau \t\n",
      "\n",
      "rhizopus \t\n",
      "\n",
      "hangzhou \ts\n",
      "\n",
      "macrosporangium \ts\n",
      "\n",
      "quantum \t\n",
      "\n",
      "anogramma \ts\n",
      "\n",
      "abscissa \t\n",
      "\n",
      "balloonfish \t\n",
      "\n",
      "luggage \t\n",
      "\n",
      "dormouse \t\n",
      "\n",
      "beau \t\n",
      "\n",
      "enema \t\n",
      "\n",
      "voodoo \ts\n",
      "\n",
      "rudderfish \t\n",
      "\n",
      "cranium \t\n",
      "\n",
      "shikoku \ts\n",
      "\n",
      "draftsperson \t\n",
      "\n",
      "opus \t\n",
      "\n",
      "cod \t\n",
      "\n",
      "datum \t\n",
      "\n",
      "lacuna \t\n",
      "\n",
      "otoganglion \t\n",
      "\n",
      "sudoku \ts\n",
      "\n",
      "bigfoot \t\n",
      "\n",
      "octopus \tes\n",
      "\n",
      "compendium \t\n",
      "\n",
      "cherub \t\n",
      "\n",
      "nexus \ts\n",
      "\n",
      "hydra \t\n",
      "\n",
      "solo \t\n",
      "\n",
      "djinn \t\n",
      "\n",
      "bema \t\n",
      "\n",
      "lemma \t\n",
      "\n",
      "organon \t\n",
      "\n",
      "pygopus \t\n",
      "\n",
      "eau \t\n",
      "\n",
      "plateau \t\n",
      "\n",
      "jawfish \t\n",
      "\n",
      "commandeer \t\n",
      "\n",
      "referendum \ts\n",
      "\n",
      "krebs \ts\n",
      "\n",
      "prolegomenon \t\n",
      "\n",
      "desideratum \t\n",
      "\n",
      "afoot \t\n",
      "\n",
      "knowledge \t\n",
      "\n",
      "chantey \ts\n",
      "\n",
      "occiput \t\n",
      "\n",
      "stadium \t\n",
      "\n",
      "enigma \t\n",
      "\n",
      "momentum \t\n",
      "\n",
      "miasma \t\n",
      "\n",
      "cagney \ts\n",
      "\n",
      "ephemeris \tes\n",
      "\n",
      "hyperbaton \t\n",
      "\n",
      "reindeer \t\n",
      "\n",
      "cellulose \t\n",
      "\n",
      "dogma \t\n",
      "\n",
      "myopus \t\n",
      "\n",
      "nimbus \t\n",
      "\n",
      "godchild \t\n",
      "\n",
      "man \t\n",
      "\n",
      "thee \t\n",
      "\n",
      "stoop \ts\n",
      "\n",
      "alumna \t\n",
      "\n",
      "bjs \ts\n",
      "\n",
      "carcinoma \t\n",
      "\n",
      "agendum \t\n",
      "\n",
      "dessertspoon \ts\n",
      "\n",
      "dictum \t\n",
      "\n",
      "focus \t\n",
      "\n",
      "aphelion \t\n",
      "\n",
      "brainchild \t\n",
      "\n",
      "velum \t\n",
      "\n",
      "sikh \ts\n",
      "\n",
      "succubus \t\n",
      "\n",
      "soupfin \ts\n",
      "\n",
      "titmouse \t\n",
      "\n",
      "spectrum \t\n",
      "\n",
      "brother \t\n",
      "\n",
      "anchorperson \t\n",
      "\n",
      "tamandu \ts\n",
      "\n",
      "pteropus \t\n",
      "\n",
      "stigma \t\n",
      "\n",
      "goldfish \t\n",
      "\n",
      "epizoon \t\n",
      "\n",
      "garbage \t\n",
      "\n",
      "vibrissa \ts\n",
      "\n",
      "sand \t\n",
      "\n",
      "rice \t\n",
      "\n",
      "vacuum \t\n",
      "\n",
      "trapezium \t\n",
      "\n",
      "noumenon \t\n",
      "\n",
      "graffito \t\n",
      "\n",
      "parabola \t\n",
      "\n",
      "Latent Dim:  5 0.975858585859\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "for i in range(1,2):\n",
    "    latent_dim = i * 5\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    print(str(datetime.datetime.now()), \"Training with \", latent_dim,\n",
    "          \" latent dimensions\")\n",
    "\n",
    "    model.load_weights('./final_ending_models/rmsprop_model_500_' + str(latent_dim) + '.h5')\n",
    "\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    reverse_input_char_index = dict(\n",
    "        (i, char) for char, i in input_token_index.items())\n",
    "    reverse_target_char_index = dict(\n",
    "        (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "    delta = 0\n",
    "    correct = 0\n",
    "    encoding_vectors = []\n",
    "    counter = []\n",
    "    for seq_index in range(len(train_input)):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying outdecoding.\n",
    "#         if seq_index % 1000 == 0:\n",
    "#             print(seq_index, str(datetime.datetime.now()), correct)\n",
    "        input_seq = train_encoder_input_data[seq_index: seq_index+ 1]\n",
    "        decoded_sentence, encoding_vector = decode_sequence(input_seq)\n",
    "        encoding_vectors.append(encoding_vector)\n",
    "\n",
    "        if decoded_sentence == train_output[seq_index][1:]:\n",
    "            correct += 1\n",
    "            counter.append(1)\n",
    "        else:\n",
    "            print(train_input[seq_index], train_output[seq_index])\n",
    "            counter.append(0)\n",
    "\n",
    "    counter = np.array(counter)\n",
    "    train_acc.append(correct/len(train_input))\n",
    "    print(\"Latent Dim: \", latent_dim, np.sum(counter)/len(train_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Dim:  5 0.974545454545\n",
      "Latent Dim:  10 0.964545454545\n",
      "Latent Dim:  15 0.970909090909\n",
      "Latent Dim:  20 0.970909090909\n",
      "Latent Dim:  25 0.976363636364\n",
      "Latent Dim:  30 0.98\n",
      "Latent Dim:  35 0.981818181818\n",
      "Latent Dim:  40 0.979090909091\n",
      "Latent Dim:  45 0.977272727273\n",
      "Latent Dim:  50 0.980909090909\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "for i in range(1, 11):\n",
    "    latent_dim = 5 * i\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "    model.load_weights('./final_ending_models/rmsprop_model_500_' + str(latent_dim) + '.h5')\n",
    "\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    reverse_input_char_index = dict(\n",
    "        (i, char) for char, i in input_token_index.items())\n",
    "    reverse_target_char_index = dict(\n",
    "        (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "    delta = 0\n",
    "    correct = 0\n",
    "    encoding_vectors = []\n",
    "    counter = []\n",
    "    for seq_index in range(len(test_input)):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying outdecoding.\n",
    "    #         if seq_index % 100 == 0:\n",
    "    #             print(seq_index, str(datetime.datetime.now()), correct)\n",
    "        input_seq = test_encoder_input_data[seq_index: seq_index+ 1]\n",
    "        decoded_sentence, encoding_vector = decode_sequence(input_seq)\n",
    "        encoding_vectors.append(encoding_vector)\n",
    "\n",
    "        if decoded_sentence == test_output[seq_index][1:]:\n",
    "            correct += 1\n",
    "            counter.append(1)\n",
    "        else:\n",
    "    #         print(test_input[seq_index], test_output[seq_index])\n",
    "            counter.append(0)\n",
    "\n",
    "    test_acc.append(correct/len(test_input))\n",
    "    counter = np.array(counter)\n",
    "    print(\"Latent Dim: \", latent_dim, np.sum(counter)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoding_vectors = np.array(encoding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900, 2, 225)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_vectors = encoding_vectors.reshape(9900, 2, 225)\n",
    "encoding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_vectors = np.hstack((encoding_vectors[:, 0, :], encoding_vectors[:, 1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treeless treelesses\n",
      "Putting 1  treelesses\n",
      "spikemoss spikemosses\n",
      "Putting 1  spikemosses\n",
      "pooch pooches\n",
      "Putting 1  pooches\n",
      "shamefulness shamefulnesses\n",
      "Putting 1  shamefulnesses\n",
      "sensuousness sensuousnesses\n",
      "Putting 1  sensuousnesses\n",
      "grimness grimnesses\n",
      "Putting 1  grimnesses\n",
      "amoebiasis amoebiases\n",
      "Putting 1  amoebiases\n",
      "hemolysis hemolyses\n",
      "Putting 1  hemolyses\n",
      "successfulness successfulnesses\n",
      "Putting 1  successfulnesses\n",
      "sternness sternnesses\n",
      "Putting 1  sternnesses\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for i in range(500, 510):\n",
    "    input_sample = test_input[i]\n",
    "    output_sample = test_output[i][1:-1]\n",
    "    print(input_sample, output_sample)\n",
    "    if (input_sample[-2:] == 'ch' or input_sample[-2:] == 'sh' or input_sample[-1:] == 'x' or \\\n",
    "    input_sample[-1:] == 's') or input_sample[-1:] == 'z':\n",
    "        labels.append(1)\n",
    "        print(\"Putting 1 \", output_sample)\n",
    "    elif input_sample[-1:] == 'y' and output_sample[-3:] == 'ies':\n",
    "        labels.append(2)\n",
    "    elif output_sample[-1:] == 's':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9758585858585859,\n",
       " 0.9744444444444444,\n",
       " 0.9785858585858586,\n",
       " 0.9708080808080808,\n",
       " 0.9817171717171718,\n",
       " 0.9865656565656565,\n",
       " 0.9842424242424243,\n",
       " 0.9891919191919192,\n",
       " 0.988989898989899,\n",
       " 0.9880808080808081]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1072, 1061, 1068, 1068, 1074, 1078, 1080, 1077, 1075, 1079]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_weights = decoder_lstm.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 120)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 120)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weights[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'activity_regularizer': None,\n",
       " 'bias_constraint': None,\n",
       " 'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       " 'bias_regularizer': None,\n",
       " 'dropout': 0.0,\n",
       " 'go_backwards': False,\n",
       " 'implementation': 1,\n",
       " 'kernel_constraint': None,\n",
       " 'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "  'config': {'distribution': 'uniform',\n",
       "   'mode': 'fan_avg',\n",
       "   'scale': 1.0,\n",
       "   'seed': None}},\n",
       " 'kernel_regularizer': None,\n",
       " 'name': 'lstm_13',\n",
       " 'recurrent_activation': 'hard_sigmoid',\n",
       " 'recurrent_constraint': None,\n",
       " 'recurrent_dropout': 0.0,\n",
       " 'recurrent_initializer': {'class_name': 'Orthogonal',\n",
       "  'config': {'gain': 1.0, 'seed': None}},\n",
       " 'recurrent_regularizer': None,\n",
       " 'return_sequences': False,\n",
       " 'return_state': True,\n",
       " 'stateful': False,\n",
       " 'trainable': True,\n",
       " 'unit_forget_bias': True,\n",
       " 'units': 30,\n",
       " 'unroll': False,\n",
       " 'use_bias': True}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.consumers of <tf.Tensor 'input_37:0' shape=(?, ?, 26) dtype=float32>>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs.consumers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es', 'es']"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['es']*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ds = train_input\n",
    "output_ds = train_output\n",
    "for i in range(len(input_ds)):\n",
    "    if input_sample[-2:] == 'ch' or input_sample[-2:] == 'sh' or input_sample[-1:] == 'x' or \\\n",
    "    input_sample[-1:] == 's' or input_sample[-1:] == 'z':\n",
    "        prediction = input_ds[i] + 'es'\n",
    "    elif input_sample[-1:] == 'y' and output_sample[-3:]:\n",
    "        prediction = input_ds[i][:-1] + 'ies'\n",
    "    elif output_sample[-1:] == 's':\n",
    "        prediction = input_ds[i] + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treeless \ttreelesses\n",
      "\n",
      "spikemoss \tspikemosses\n",
      "\n",
      "pooch \tpooches\n",
      "\n",
      "shamefulness \tshamefulnesses\n",
      "\n",
      "sensuousness \tsensuousnesses\n",
      "\n",
      "grimness \tgrimnesses\n",
      "\n",
      "amoebiasis \tamoebiases\n",
      "\n",
      "hemolysis \themolyses\n",
      "\n",
      "successfulness \tsuccessfulnesses\n",
      "\n",
      "sternness \tsternnesses\n",
      "\n",
      "comradeliness \tcomradelinesses\n",
      "\n",
      "dispiritedness \tdispiritednesses\n",
      "\n",
      "ignominiousness \tignominiousnesses\n",
      "\n",
      "fearlessness \tfearlessnesses\n",
      "\n",
      "invalidness \tinvalidnesses\n",
      "\n",
      "breezelessness \tbreezelessnesses\n",
      "\n",
      "unscrupulousness \tunscrupulousnesses\n",
      "\n",
      "insidiousness \tinsidiousnesses\n",
      "\n",
      "landless \tlandlesses\n",
      "\n",
      "shortgrass \tshortgrasses\n",
      "\n",
      "niceness \tnicenesses\n",
      "\n",
      "acridness \tacridnesses\n",
      "\n",
      "loquaciousness \tloquaciousnesses\n",
      "\n",
      "turbidness \tturbidnesses\n",
      "\n",
      "reconditeness \treconditenesses\n",
      "\n",
      "groundmass \tgroundmasses\n",
      "\n",
      "ilx \tilxes\n",
      "\n",
      "limax \tlimaxes\n",
      "\n",
      "clumsiness \tclumsinesses\n",
      "\n",
      "acariasis \tacariases\n",
      "\n",
      "abimelech \tabimeleches\n",
      "\n",
      "collectedness \tcollectednesses\n",
      "\n",
      "braveness \tbravenesses\n",
      "\n",
      "ogress \togresses\n",
      "\n",
      "incorruptness \tincorruptnesses\n",
      "\n",
      "industriousness \tindustriousnesses\n",
      "\n",
      "bibless \tbiblesses\n",
      "\n",
      "precipitousness \tprecipitousnesses\n",
      "\n",
      "joylessness \tjoylessnesses\n",
      "\n",
      "expeditiousness \texpeditiousnesses\n",
      "\n",
      "bountifulness \tbountifulnesses\n",
      "\n",
      "superciliousness \tsuperciliousnesses\n",
      "\n",
      "vex \tvexes\n",
      "\n",
      "sandglass \tsandglasses\n",
      "\n",
      "wretchedness \twretchednesses\n",
      "\n",
      "musquash \tmusquashes\n",
      "\n",
      "pneumovax \tpneumovaxes\n",
      "\n",
      "hemogenesis \themogeneses\n",
      "\n",
      "disconnectedness \tdisconnectednesses\n",
      "\n",
      "transitiveness \ttransitivenesses\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy of LSTM seq2seq model on predicting full pluralized word')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEWCAYAAAApTuNLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb8ad69668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = [0.1986868686868687,\n",
    " 0.8527272727272728,\n",
    " 0.8881818181818182,\n",
    " 0.9974747474747475,\n",
    " 0.9927272727272727,\n",
    " 0.9947474747474747,\n",
    " 0.9949494949494949,\n",
    " 0.9951515151515151,\n",
    " 0.9935353535353535,\n",
    " 0.9956565656565657]\n",
    "\n",
    "test_acc = [ 0.18727273,  0.75      ,  0.75818182,  0.86909091,  0.84636364,\n",
    "    0.86181818,  0.86909091,  0.83454545,  0.84909091,  0.83181818]\n",
    "\n",
    "plt.plot(range(1, 11), train_acc, 'r', label=\"Train\")\n",
    "plt.plot(range(1, 11), train_acc, 'ro')\n",
    "\n",
    "plt.plot(range(1, 11), test_acc, 'b', label=\"Test\")\n",
    "plt.plot(range(1, 11), test_acc, 'bo')\n",
    "\n",
    "characters = [str(i*25) for i in range(1, 11)]\n",
    "plt.xticks(range(1, 11), characters)\n",
    "plt.xlabel('Number of neurons')\n",
    "plt.ylabel('Accuracy (max is 1.0)')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Accuracy of LSTM seq2seq model on predicting full pluralized word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_acc = [0.9758585858585859,\n",
    " 0.9744444444444444,\n",
    " 0.9785858585858586,\n",
    " 0.9708080808080808,\n",
    " 0.9817171717171718,\n",
    " 0.9865656565656565,\n",
    " 0.9842424242424243,\n",
    " 0.9891919191919192,\n",
    " 0.988989898989899,\n",
    " 0.9880808080808081]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9745454545454545,\n",
       " 0.9645454545454546,\n",
       " 0.9709090909090909,\n",
       " 0.9709090909090909,\n",
       " 0.9763636363636363,\n",
       " 0.98,\n",
       " 0.9818181818181818,\n",
       " 0.9790909090909091,\n",
       " 0.9772727272727273,\n",
       " 0.980909090909091]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
